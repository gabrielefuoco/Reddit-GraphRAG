
## 2. Architettura del Sistema

L'architettura del progetto è stata concepita per essere modulare, robusta e scalabile, separando nettamente le operazioni di elaborazione batch (offline) da quelle di interrogazione in tempo reale (online). Questa separazione garantisce che la costruzione del knowledge graph, un processo computazionalmente intensivo, non impatti le performance dell'applicazione web con cui l'utente finale interagisce. L'intero sistema è orchestrato da script Python e si appoggia a un ecosistema di librerie open-source specializzate.

Il diagramma seguente offre una visione d'insieme del flusso dei dati e delle componenti architetturali:

![[Untitled diagram-2025-10-31-172240.png|908x1581]]

###  Dalla Rete al Grafo

La fase di estrazione è il cuore del sistema, responsabile della trasformazione dei dati grezzi di Reddit in un knowledge graph strutturato e semanticamente ricco. Questo processo è orchestrato dallo script `run_pipeline.py`, che esegue in sequenza i vari stadi della pipeline ETL (Extract, Transform, Load) e di analisi.

#### 2.1.1. Ingestione e Arricchimento dei Dati (`src.graph.builder`)

Il processo inizia con l'ingestione dei dati da Reddit.

- **Estrazione Asincrona (`src.data_processing.ingestion.py`)**: Utilizzando la libreria `asyncpraw`, il sistema si connette all'API di Reddit per scaricare i post e i relativi commenti in modo asincrono. Questo approccio garantisce un'elevata efficienza, permettendo di parallelizzare le richieste di rete. Vengono recuperati i post più popolari ("top") dell'ultimo anno da una lista predefinita di subreddit a tema politico. Per ogni post, vengono estratti anche i commenti più votati, fornendo così un dataset ricco di interazioni.
    
- **Pulizia del Testo (`src.data_processing.cleaner.py`)**: Ogni testo (sia di post che di commenti) viene sottoposto a un rigoroso processo di pulizia. Utilizzando `spaCy`, il testo viene normalizzato (convertito in minuscolo), privato di URL e caratteri speciali, e infine lemmatizzato. La lemmatizzazione, che riduce le parole alla loro forma base (es. "running" -> "run"), è fondamentale per consolidare il significato semantico e migliorare la qualità dell'analisi successiva.
    
- **Analisi NLP (`src.nlp.analysis.py`)**: Questa è la fase di arricchimento semantico, dove il testo pulito viene trasformato in informazione strutturata.
    
    - **Generazione di Embedding**: Per ogni testo, viene generato un embedding vettoriale utilizzando il modello `sentence-transformers/all-mpnet-base-v2`. Questi vettori, che catturano il significato semantico del testo, sono cruciali per la successiva ricerca di similarità e per le analisi semantiche.
        
    - **Estrazione di Entità (NER)**: Attraverso chiamate a un LLM (Large Language Model) locale gestito da Ollama (`qwen3:4b-instruct-2507-q4_K_M`), il sistema identifica le entità politiche (persone, organizzazioni, concetti) menzionate nel testo.
      Per l'inferenza dei modelli LLM si è optato per un approccio locale tramite Ollama, garantendo così il pieno controllo sulla catena di elaborazione, la privacy dei dati e l'indipendenza da API di terze parti a pagamento
        
    - **Rilevamento della Stance**: Per ogni coppia (testo, entità), viene eseguita un'analisi della stance per determinare se il testo esprime un'opinione `FAVORABLE`, `AGAINST`, o `NEUTRAL` verso l'entità. Per i commenti, questa analisi è contestuale: il contenuto del post genitore viene fornito all'LLM per aiutarlo a interpretare correttamente il sarcasmo o i riferimenti impliciti. Un semaforo asincrono (`asyncio.Semaphore`) viene utilizzato per limitare il numero di chiamate concorrenti all'LLM, prevenendo così il sovraccarico del servizio.
        
- **Caricamento nel Grafo (`src.graph.builder.py`)**: I dati, ora arricchiti, vengono caricati in Neo4j. Il `GraphBuilder` utilizza transazioni Cypher ottimizzate per creare o aggiornare i nodi (`Post`, `Comment`, `User`, `PoliticalEntity`) e le relative relazioni (`POSTED`, `REPLY_TO`, `MENTIONS`, `HAS_STANCE`), popolando il database in modo efficiente e robusto.
	- È stato scelto Neo4j per il suo maturo supporto al linguaggio di query Cypher e per l'integrazione nativa con la libreria Graph Data Science (GDS), preferendolo ad alternative che potrebbero richiedere una maggiore complessità architetturale per l'analisi dei grafi.
    

#### 2.1.2. Struttura del Grafo (`src.graph.schemas.py` e `schema_setup.py`)

Lo schema del grafo è progettato per modellare in modo intuitivo l'ecosistema di Reddit.

- **Nodi Principali**:
    
    - `Post`, `Comment`: Contengono il testo originale, quello pulito, l'embedding vettoriale e metadati come l'autore, il punteggio e il timestamp.
        
    - `User`: Rappresenta un autore di Reddit.
        
    - `PoliticalEntity`: Rappresenta un'entità politica menzionata.
        
    - `IdeologicalSummary`: Un nodo speciale che conterrà i riassunti generati dall'analisi delle ideologie.
        
- **Relazioni**:
    
    - `(:User)-[:POSTED]->(:Post)`
        
    - `(:Comment)-[:REPLY_TO]->(:Post)`
        
    - `(:Post)-[:MENTIONS]->(:PoliticalEntity)`
        
    - `(:Post)-[:HAS_STANCE]->(:PoliticalEntity)`: Questa relazione è arricchita con le proprietà `stance` e `confidence`.
        
- **Indici e Vincoli**: Lo script `src/graph/schema_setup.py` si occupa di creare i vincoli di unicità (es. ID dei post, nomi degli utenti) e gli indici necessari per ottimizzare le query. In particolare, vengono creati indici full-text per la ricerca testuale e indici vettoriali sulla proprietà `embedding` dei nodi `Post`, `Comment` e `IdeologicalSummary`, abilitando interrogazioni di similarità semantica ultra-veloci.
    

#### 2.1.3. Trasformazione e Analisi del Grafo

Una volta che il grafo è stato popolato, vengono eseguiti ulteriori passaggi di trasformazione e analisi per raffinarne la struttura e derivare insight di livello superiore.

- **Defragmentazione delle Entità (`src.scripts.defragment_entities.py`, `src.scripts.merge_entities.py`)**: È comune che le stesse entità vengano menzionate con nomi leggermente diversi (es. "Joe Biden", "Biden", "President Biden"). Per consolidare questi riferimenti, viene eseguito un processo di defragmentazione.
    
    1. Tutte le entità uniche vengono estratte dal grafo.
        
    2. Viene utilizzata la libreria `rapidfuzz` per calcolare la similarità tra i nomi delle entità.
        
    3. Un algoritmo di clustering gerarchico raggruppa i nomi simili.
        
    4. Per ogni cluster, viene scelto un nome "canonico" (tipicamente il più lungo e descrittivo).
        
    5. Viene generato un file `canonical_map.json` che mappa ogni alias al suo nome canonico.
        
    6. Infine, lo script `merge_entities` utilizza questa mappa e la procedura `apoc.refactor.mergeNodes` di APOC per unire i nodi duplicati nel grafo, trasferendo tutte le relazioni al nodo canonico.
        
- **Analisi delle Comunità e Riassunti (`src.scripts.run_analysis.py`)**: Questa fase utilizza la libreria Graph Data Science (GDS) di Neo4j per analisi avanzate.
    
    1. **Creazione del Grafo delle Alleanze**: Viene creato un grafo virtuale in memoria in cui i nodi sono gli `User` e una relazione `AGREES_WITH` collega due utenti se hanno espresso la stessa stance (con un'alta soglia di confidenza) sulla stessa entità politica. Il peso della relazione è proporzionale al numero di entità su cui concordano.
        
    2. **Community Detection (`src.graph.gds_analyzer.py`)**: Sul grafo delle alleanze viene eseguito l'algoritmo di Leiden, un metodo di community detection all'avanguardia che identifica cluster di utenti con allineamenti ideologici simili. L'ID della comunità viene scritto come proprietà su ciascun nodo `User` nel grafo principale.
        
    3. **Riassunto delle Ideologie (`src.graph.analyzer.summarizer.py`)**: Per ogni ideologia significativa (coppia di `PoliticalEntity` e `Stance` con sufficiente supporto), il sistema costruisce un "dossier" testuale combinando i post più rilevanti e i commenti di supporto. Questo dossier viene poi passato a un LLM per generare un riassunto analitico, che viene salvato come nodo `IdeologicalSummary` nel grafo, completo del suo embedding vettoriale.
        

### 2.2. Pipeline Online: Interrogazione e Interazione

La fase online è progettata per offrire un'esperienza utente fluida e reattiva, nascondendo la complessità della pipeline di interrogazione sottostante.

- **Interfaccia Utente (`app.py`)**: L'applicazione, sviluppata in Streamlit, funge da punto di ingresso. Fornisce due viste principali: una "Chat" per l'interazione conversazionale con l'agente e una "Panoramica Entità" per l'esplorazione visuale dei dati, che mostra la distribuzione delle stance e un grafo delle menzioni per un'entità selezionata. La connessione al database e l'inizializzazione dell'agente sono gestite tramite la cache di Streamlit (`@st.cache_resource`) per ottimizzare le performance.
    
- **Agente Conversazionale (`src.agent.react_agent.py`)**: Il cervello del sistema di interrogazione è un agente basato sul framework ReAct (Reasoning and Acting) di LangChain. Questo agente è in grado di ragionare sulla richiesta dell'utente, scegliere lo strumento più appropriato da un set predefinito e pianificare le azioni necessarie per formulare una risposta. Il prompt dell'agente è attentamente progettato per guidare il suo processo di pensiero e per gestire la cronologia della conversazione, consentendo un dialogo contestuale.
    
- **La Toolchain dell'Agente**: L'agente ha a disposizione un unico ma potente strumento: la `GraphRAGPipeline`. Questo incapsula tutta la logica di interrogazione del knowledge graph.
    
- **Pipeline GraphRAG (`src.pipeline.rag_chain.py`)**: Questa pipeline implementa la strategia di recupero ibrido. Quando interrogata, esegue i seguenti passaggi:
    
    1. **Estrazione Entità e Intento**: Analizza la query dell'utente per estrarre le entità politiche menzionate e per determinare un eventuale "intento di stance" (ad esempio, se la domanda chiede opinioni positive o negative).
        
    2. **Recupero Gerarchico**: Se vengono trovate entità, esegue una query strutturata su Neo4j. In primo luogo, cerca i nodi `IdeologicalSummary` pertinenti. Successivamente, recupera i `Post` individuali che menzionano le entità, ordinandoli per punteggio. Questo approccio a più livelli, che parte dai riassunti di alto livello per poi scendere ai dati grezzi, fornisce un contesto ricco e sfaccettato.
        
    3. **Fallback Semantico**: Se il recupero gerarchico non produce risultati (ad esempio, per una query molto generica che non menziona entità specifiche), la pipeline non si arrende. Calcola l'embedding vettoriale della query dell'utente ed esegue una ricerca di similarità vettoriale (ANN - Approximate Nearest Neighbor) sull'indice dei post in Neo4j. Questo "fallback dinamico" garantisce che il sistema possa fornire risultati pertinenti anche quando la ricerca strutturata fallisce.
        
    4. **Generazione della Risposta**: Il contesto recuperato (sia dal recupero gerarchico che dal fallback semantico) viene formattato e passato, insieme alla query originale, al `rag_chain`. Un LLM (`qwen3:4b-instruct-2507-q8_0`) sintetizza queste informazioni in una risposta coerente e in linguaggio naturale, che viene infine restituita all'agente e visualizzata nell'interfaccia utente.
        

Questa architettura a due fasi, con una chiara distinzione tra elaborazione offline e interrogazione online, consente di sfruttare appieno la potenza dei knowledge graph e degli LLM, creando un sistema di analisi politica che è al contempo profondo, efficiente e facile da usare.