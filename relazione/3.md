## 3. Pipeline dei Dati e Creazione del Dataset

La fondazione di qualsiasi sistema di analisi dati risiede nella qualità e nella coerenza del dataset sottostante. Per questo progetto, è stata sviluppata una pipeline di dati robusta e multifase che gestisce l'intero ciclo di vita del dato: dall'acquisizione grezza dalla fonte, passando per la pulizia e l'arricchimento semantico, fino al caricamento in una struttura a grafo ottimizzata per l'analisi. Questa pipeline, orchestrata dallo script `run_pipeline.py`, è progettata per essere rieseguibile e modulare.

### 3.1. Fase di Estrazione (Extract): Acquisizione Dati da Reddit

La prima fase del processo consiste nell'acquisire i dati testuali direttamente dalla piattaforma Reddit.

* **Fonte dei Dati**: Il dataset è costruito a partire da una selezione di subreddit noti per le loro discussioni a tema politico, tra cui `r/politics`, `r/PoliticalDiscussion`, `r/Conservative`, `r/Liberal`, `r/antiwork`, e `r/changemyview`. Questa selezione mira a catturare un ventaglio di prospettive diverse, sebbene non necessariamente rappresentativo dell'intero spettro politico.
* **Strumento di Ingestione**: Per interagire con l'API di Reddit, è stata utilizzata la libreria `asyncpraw`. La scelta di un approccio asincrono è strategica: consente di eseguire più richieste di rete in parallelo, massimizzando il throughput e riducendo significativamente il tempo totale di acquisizione dei dati.
* **Logica di Estrazione**: Lo script `src/data_processing/ingestion.py` implementa la logica per recuperare i `POST_LIMIT` (impostato a 250) post più votati (`top`) dell'ultimo anno per ciascun subreddit. Per ogni post, vengono scaricati anche i commenti "top-level" più rilevanti (fino a un massimo di 20, ordinati per punteggio), in modo da catturare non solo il contenuto iniziale ma anche le reazioni e le discussioni che ne derivano. Vengono estratti metadati cruciali come l'ID del post/commento, l'autore, il contenuto testuale, il timestamp di creazione, il punteggio (upvotes - downvotes) e il subreddit di origine.

### 3.2. Fase di Trasformazione (Transform): Pulizia e Arricchimento Semantico

Una volta acquisiti, i dati grezzi vengono sottoposti a un'intensa fase di trasformazione per pulirli dal rumore e arricchirli con informazioni strutturate e semantiche. Questa fase è cruciale per la qualità del knowledge graph finale.

#### 3.2.1. Pulizia e Normalizzazione del Testo

Il modulo `src/data_processing/cleaner.py` è dedicato a questa operazione preliminare ma fondamentale. Ogni stringa di testo viene processata come segue:
1.  **Rimozione URL**: Tutti i link HTTP/HTTPS vengono rimossi tramite espressioni regolari per eliminare contenuti esterni non pertinenti.
2.  **Normalizzazione**: Il testo viene convertito in minuscolo per garantire la coerenza.
3.  **Tokenizzazione, Lemmatizzazione e Rimozione Stopword**: Utilizzando una pipeline `spaCy` (`en_core_web_sm`), il testo viene suddiviso in token. Le stopword (parole comuni come "the", "a", "is") e la punteggiatura vengono rimosse. Infine, ogni token viene ridotto alla sua forma base (lemma). Questo passaggio consolida il vocabolario e riduce la dimensionalità dei dati, migliorando l'efficacia delle analisi successive.

#### 3.2.2. Arricchimento Semantico tramite NLP

Il modulo `src/nlp/analysis.py` orchestra l'applicazione di modelli di machine learning per estrarre insight profondi dal testo pulito.

* **Generazione di Embedding Vettoriali**: Ogni documento (post o commento) viene trasformato in un vettore numerico a 768 dimensioni utilizzando il modello `sentence-transformers/all-mpnet-base-v2`. Questo embedding cattura il significato semantico del testo, permettendo di misurare la similarità tra documenti in modo molto più efficace rispetto alle parole chiave. L'operazione è eseguita in batch per ottimizzare l'uso della GPU/CPU.

* **Estrazione di Entità Nominate (NER)**: Il sistema identifica le entità politiche rilevanti all'interno del testo. Questa operazione non si affida a un modello NER tradizionale pre-addestrato, ma utilizza un approccio basato su LLM (Large Language Model) con un prompt ingegnerizzato ad hoc (`entity_prompt_template`). Il prompt istruisce il modello `qwen3:4b-instruct-2507-q4_K_M`, servito tramite Ollama, a estrarre "figure politiche, organizzazioni o concetti politici specifici" e a restituire l'output in un formato JSON strutturato. Questo approccio offre una maggiore flessibilità rispetto ai modelli NER classici.

* **Rilevamento della Stance (Stance Detection)**: Per ogni entità estratta, il sistema ne determina la stance espressa nel testo. Anche in questo caso, si utilizza un LLM con prompt specifici:
    * **Stance Semplice (`stance_prompt_template`)**: Determina se la stance verso un'entità in un dato testo è `FAVORABLE`, `AGAINST`, o `NEUTRAL`.
    * **Stance Contestuale (`stance_contextual_prompt_template`)**: Una variante più sofisticata utilizzata per i commenti. Oltre al testo del commento, al prompt viene fornito anche il contenuto del post originale come contesto. Questo permette all'LLM di interpretare meglio il sarcasmo, l'ironia e i riferimenti impliciti, che sono comuni nelle discussioni online.
    Ogni classificazione di stance è accompagnata da un punteggio di confidenza (`confidence`), fondamentale per filtrare le previsioni a bassa affidabilità nelle fasi successive di analisi.

Per gestire l'elevato numero di chiamate all'LLM richieste da queste operazioni, è stato implementato un meccanismo di controllo della concorrenza tramite un `asyncio.Semaphore`, che limita il numero di richieste simultanee per evitare di sovraccaricare il servizio Ollama.

### 3.3. Fase di Caricamento (Load): Costruzione del Knowledge Graph

L'ultima fase della pipeline offline consiste nel caricare i dati, ora puliti e arricchiti, nel database Neo4j.

* **Validazione dei Dati con Pydantic**: Prima del caricamento, i dati arricchiti vengono validati rispetto a uno schema rigoroso definito tramite modelli Pydantic (`Post`, `Comment`, `PoliticalEntity`, `Stance`) nel file `src/graph/schemas.py`. Questo approccio garantisce l'integrità e la coerenza dei dati che entrano nel grafo, prevenendo errori e incongruità.

* **Costruzione del Grafo (`src.graph.builder.py`)**: Il modulo `GraphBuilder` gestisce l'intero processo. I dati vengono processati in "mini-batch" per gestire la memoria in modo efficiente. Per ogni batch, viene costruita una singola, grande transazione Cypher che esegue le seguenti operazioni in modo atomico:
    1.  Utilizza `MERGE` per creare o aggiornare i nodi `:Post`, `:Comment`, `:User` e `:PoliticalEntity`, usando i loro ID unici come chiave. Questo previene la creazione di duplicati.
    2.  Imposta le proprietà dei nodi, inclusi il contenuto, i metadati e l'embedding vettoriale.
    3.  Crea le relazioni tra i nodi: `(:User)-[:POSTED]->(:Post)`, `(:Comment)-[:REPLY_TO]->(:Post)`, `(:Post)-[:MENTIONS]->(:PoliticalEntity)`.
    4.  Crea la relazione `(:Post)-[:HAS_STANCE]->(:PoliticalEntity)`, arricchendola con le proprietà `stance` e `confidence` estratte in precedenza.

* **Gestione degli Errori e Dead Letter Queue (DLQ)**: La pipeline è progettata per essere resiliente. In caso di fallimento durante l'elaborazione di un singolo post o commento (ad esempio, a causa di una validazione Pydantic fallita o di un errore imprevisto), l'elemento problematico viene salvato in una "dead letter queue" (una directory su disco) insieme alla motivazione dell'errore. Questo impedisce che un singolo dato corrotto blocchi l'intera pipeline di ingestione, permettendo un'analisi post-mortem degli errori.

Il risultato di questa pipeline è un dataset ricco, strutturato come un knowledge graph, che funge da solida base di conoscenza per la fase di interrogazione online del sistema.