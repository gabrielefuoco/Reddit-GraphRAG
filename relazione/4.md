## 4. Il Sistema RAG Sviluppato: Enfasi sul GraphRAG

Il cuore della fase di interrogazione online del progetto è rappresentato dalla sua innovativa pipeline di Retrieval-Augmented Generation (RAG). A differenza degli approcci RAG tradizionali, che si basano quasi esclusivamente su database vettoriali per il recupero delle informazioni, questo sistema implementa un'architettura **GraphRAG ibrida**. Questo approccio sfrutta la ricchezza strutturale del knowledge graph per guidare il processo di recupero, offrendo un contesto più profondo, accurato e sfaccettato all'LLM per la generazione della risposta finale.

### 4.1. Limiti del RAG Tradizionale e Vantaggi del GraphRAG

I sistemi RAG convenzionali tipicamente funzionano in due fasi:
1.  **Retrieval**: Data una query utente, il sistema calcola il suo embedding vettoriale e lo utilizza per trovare i "chunk" di testo più semanticamente simili all'interno di un database vettoriale.
2.  **Generation**: I chunk di testo recuperati vengono inseriti in un prompt insieme alla query originale e passati a un LLM, che sintetizza le informazioni per generare una risposta.

Sebbene efficace, questo approccio presenta delle limitazioni, specialmente in domini complessi come l'analisi politica:
* **Mancanza di Contesto Relazionale**: Una ricerca vettoriale pura tratta ogni chunk di testo come un'isola. Non è in grado di comprendere o sfruttare le relazioni implicite o esplicite tra i documenti, come ad esempio chi sta parlando, a chi sta rispondendo, o quali entità sono coinvolte in una discussione.
* **Rischio di Recupero Irrilevante**: La similarità semantica non sempre equivale a pertinenza. Una query potrebbe essere semanticamente simile a un documento che, tuttavia, non risponde alla domanda specifica dell'utente.
* **Difficoltà con Query Complesse**: Le domande che richiedono di aggregare informazioni basate su relazioni (es. "Quali sono le principali critiche a X da parte degli utenti della comunità Y?") sono difficili da risolvere con una semplice ricerca di similarità.

Il **GraphRAG** affronta direttamente questi problemi integrando un knowledge graph nel processo di retrieval. Il grafo non è solo un contenitore di dati, ma una struttura di conoscenza attiva che modella le relazioni tra le entità. Questo permette di:
* **Navigare le Relazioni**: Eseguire query che attraversano il grafo per trovare informazioni basate su connessioni esplicite (es. trovare tutti i post di un utente che menzionano un'entità con una stance negativa).
* **Recuperare Contesto Ricco**: Invece di recuperare semplici chunk di testo, è possibile recuperare interi sotto-grafi contestuali, fornendo all'LLM una visione più olistica dell'argomento.
* **Migliorare la Pertinenza**: Ancorare il processo di retrieval a entità e relazioni specifiche riduce il rumore e aumenta la probabilità che il contesto recuperato sia direttamente pertinente alla domanda dell'utente.

### 4.2. La `GraphRAGPipeline`: Una Strategia Ibrida a Due Stadi

Il progetto implementa questa filosofia attraverso la classe `GraphRAGPipeline`. Questa pipeline non si limita a sostituire la ricerca vettoriale con quella su grafo, ma le combina in una strategia sequenziale intelligente e robusta, definita "recupero gerarchico con fallback dinamico".

#### 4.2.1. Passo 1: Analisi della Query e Rilevamento dell'Intento

Ogni query dell'utente viene prima analizzata per estrarne il significato e l'intento.
* **Estrazione delle Entità**: La query viene passata alla `entity_chain` per identificare le entità politiche menzionate. Questo è il primo e più importante passo per ancorare la ricerca al grafo.
* **Rilevamento dell'Intento di Stance**: Successivamente, la pipeline tenta di determinare se la query ha un "intento di stance", ovvero se sta chiedendo opinioni specificamente favorevoli o contrarie. Ad esempio, una domanda come "Perché le persone criticano X?" ha un chiaro intento "AGAINST". Questo viene fatto riutilizzando la `stance_chain` per classificare la stance della query stessa rispetto alla prima entità trovata.

#### 4.2.2. Passo 2: Recupero Gerarchico (Approccio Grafo-First)

Se dall'analisi della query sono state estratte una o più entità, la pipeline attiva la sua strategia di recupero principale, basata sul grafo. Questo recupero è definito "gerarchico" perché cerca di ottenere il contesto a diversi livelli di astrazione, dal più generale al più specifico.

1.  **Recupero dei Riassunti Ideologici**: La prima query eseguita cerca nel grafo i nodi `:IdeologicalSummary`. Questi nodi, creati durante la fase di analisi offline, rappresentano sintesi di alto livello delle diverse posizioni ideologiche (stance) su una specifica entità. Se la query aveva un intento di stance, la ricerca viene filtrata per recuperare solo i riassunti corrispondenti (es. solo i riassunti `AGAINST` per la query "critiche a X"). Questo fornisce immediatamente all'LLM un contesto riassuntivo e analitico.

2.  **Recupero dei Post Individuali**: Successivamente, la pipeline recupera i singoli nodi `:Post` che sono direttamente collegati alle entità identificate tramite la relazione `:HAS_STANCE`. Anche questa query viene filtrata per l'eventuale intento di stance. I post vengono ordinati per punteggio (`score`), in modo da privilegiare i contenuti considerati più rilevanti dalla comunità di Reddit. Questi post forniscono esempi concreti e citazioni dirette che arricchiscono il contesto.

Questo approccio combinato (riassunti + post individuali) assicura che il contesto fornito all'LLM sia sia **ampio** (grazie ai riassunti) che **profondo** (grazie ai post specifici).

#### 4.2.3. Passo 3: Fallback Semantico Dinamico (Approccio Vector-First)

La robustezza del sistema risiede nella sua capacità di gestire query che non si adattano bene al recupero strutturato. Se il recupero gerarchico non produce alcun risultato (ad esempio, perché la query è molto generica e non menziona entità presenti nel grafo), la pipeline attiva dinamicamente un meccanismo di fallback.

1.  **Calcolo dell'Embedding della Query**: L'embedding vettoriale della query dell'utente viene calcolato utilizzando lo stesso modello `sentence-transformers` usato per i documenti.
2.  **Ricerca Vettoriale in Neo4j**: Viene eseguita una query `db.index.vector.queryNodes` sull'indice vettoriale dei nodi `:Post` in Neo4j. Questa query trova i `top_k` post i cui embedding sono più vicini (in termini di similarità coseno) a quello della query dell'utente.
3.  **Costruzione del Contesto di Fallback**: I post recuperati tramite questa ricerca semantica diventano il contesto per la generazione della risposta.

Questo meccanismo di fallback garantisce che il sistema possa sempre tentare di fornire una risposta pertinente, anche quando la struttura del grafo non può essere sfruttata direttamente, combinando il meglio di entrambi i mondi: la precisione del grafo e la copertura della ricerca semantica.

### 4.3. Generazione della Risposta Finale

Indipendentemente da quale strategia di recupero sia stata utilizzata (gerarchica o di fallback), il contesto risultante viene formattato in una stringa di testo strutturata. Questa stringa, insieme alla domanda originale dell'utente, viene inserita nel `rag_prompt_template`.

Il prompt istruisce l'LLM (`qwen3:4b-instruct-2507-q8_0`) ad agire come un "analista politico" e a rispondere alla domanda **utilizzando esclusivamente le informazioni fornite nel contesto**. Questa istruzione è fondamentale per mitigare il rischio di "allucinazioni" e per garantire che le risposte siano ancorate ai dati del knowledge graph. Il prompt incoraggia inoltre l'LLM a integrare citazioni brevi e verbatim dal contesto per rafforzare la sua argomentazione, producendo una risposta finale che è dettagliata, supportata da prove e scritta in un linguaggio naturale e professionale. La risposta generata viene quindi passata all'agente, che la fornisce all'utente finale tramite l'interfaccia Streamlit.